{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer_Gilles import *\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "\n",
    "\n",
    "class nlp_preprocessor:\n",
    "   \n",
    "    def __init__(self, vectorizer=CountVectorizer(), tokenizer=None, \n",
    "                 cleaning_function=None, stemmer=None):\n",
    "        \"\"\"\n",
    "        A class for pipelining our data in NLP problems. The user provides a series of \n",
    "        tools, and this class manages all of the training, transforming, and modification\n",
    "        of the text data.\n",
    "        ---\n",
    "        Inputs:\n",
    "        vectorizer: the model to use for vectorization of text data\n",
    "        tokenizer: The tokenizer to use, if none defaults to split on spaces\n",
    "        cleaning_function: how to clean the data, if None, defaults to the in built class\n",
    "        \"\"\"\n",
    "        if not tokenizer:\n",
    "            tokenizer = self.splitter\n",
    "        if not cleaning_function:\n",
    "            cleaning_function = self.clean_text\n",
    "        self.stemmer = stemmer\n",
    "        self.tokenizer = tokenizer\n",
    "        #self.model = model\n",
    "        self.cleaning_function = cleaning_function\n",
    "        self.vectorizer = vectorizer\n",
    "        self._is_fit = False\n",
    "        \n",
    "    def splitter(self, text):\n",
    "        \"\"\"\n",
    "        Default tokenizer that splits on spaces naively\n",
    "        \"\"\"\n",
    "        return text.split(' ')\n",
    "        \n",
    "    def clean_text(self, text, tokenizer, stemmer):\n",
    "        \"\"\"\n",
    "        A naive function to lowercase all works can clean them quickly.\n",
    "        This is the default behavior if no other cleaning function is specified\n",
    "        \"\"\"\n",
    "        cleaned_text = []\n",
    "        for post in text:\n",
    "            cleaned_words = []\n",
    "            for word in tokenizer(post):\n",
    "                low_word = word.lower()\n",
    "                if stemmer:\n",
    "                    low_word = stemmer.stem(low_word)\n",
    "                cleaned_words.append(low_word)\n",
    "            cleaned_text.append(' '.join(cleaned_words))\n",
    "        return cleaned_text\n",
    "    \n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        Cleans the data and then fits the vectorizer with\n",
    "        the user provided text\n",
    "        \"\"\"\n",
    "        clean_text = self.cleaning_function(text, self.tokenizer, self.stemmer)\n",
    "        self.vectorizer.fit(clean_text)\n",
    "        self._is_fit = True\n",
    "        \n",
    "    def transform(self, text):\n",
    "        \"\"\"\n",
    "        Cleans any provided data and then transforms the data into\n",
    "        a vectorized format based on the fit function. Returns the\n",
    "        vectorized form of the data.\n",
    "        \"\"\"\n",
    "        if not self._is_fit:\n",
    "            raise ValueError(\"Must fit the models before transforming!\")\n",
    "        clean_text = self.cleaning_function(text, self.tokenizer, self.stemmer) # ????????? not defined here\n",
    "        return self.vectorizer.transform(clean_text)\n",
    "    \n",
    "    def save_pipe(self, filename):\n",
    "        \"\"\"\n",
    "        Writes the attributes of the pipeline to a file\n",
    "        allowing a pipeline to be loaded later with the\n",
    "        pre-trained pieces in place.\n",
    "        \"\"\"\n",
    "        if type(filename) != str:\n",
    "            raise TypeError(\"filename must be a string\")\n",
    "        pickle.dump(self.__dict__, open(filename+\".mdl\",'wb'))\n",
    "        \n",
    "    def load_pipe(self, filename):\n",
    "        \"\"\"\n",
    "        Writes the attributes of the pipeline to a file\n",
    "        allowing a pipeline to be loaded later with the\n",
    "        pre-trained pieces in place.\n",
    "        \"\"\"\n",
    "        if type(filename) != str:\n",
    "            raise TypeError(\"filename must be a string\")\n",
    "        if filename[-4:] != '.mdl':\n",
    "            filename += '.mdl'\n",
    "        self.__dict__ = pickle.load(open(filename,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_words ['bob', '123builder']\n",
      "cleaned_words ['strang']\n",
      "cleaned_words ['cartoon', 'type', 'thing']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bob 123builder', 'strang', 'cartoon type thing']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import porter\n",
    "import string\n",
    "stemmer = porter.PorterStemmer()\n",
    "\n",
    "stopwords = stopwords.words()\n",
    "def cleaning_text(text):\n",
    "    cleaned_text = []\n",
    "    for post in text:\n",
    "        cleaned_words = []\n",
    "        post = post.replace(\"â€™\",'')\n",
    "        for punc in string.punctuation:\n",
    "            post = post.replace(punc,'')\n",
    "#        print(\"post2\",post)\n",
    "        for word in post.split():\n",
    "            low_word = stemmer.stem(word.lower())\n",
    "#            print(\"low_word\",low_word)\n",
    "            if low_word not in stopwords:\n",
    "                cleaned_words.append(low_word)\n",
    "        print(\"cleaned_words\",cleaned_words)\n",
    "        cleaned_text.append(' '.join(cleaned_words))\n",
    "    return cleaned_text\n",
    "\n",
    "clean_text(['BOB the 123builder', 'is a strange', 'caRtoon type thing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nlp = nlp_preprocessor(vectorizer=TfidfVectorizer(min_df=0.3, max_df=0.8), \n",
    "                       cleaning_function=cleaning_text, \n",
    "                       tokenizer=TreebankWordTokenizer().tokenize, stemmer=PorterStemmer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./data/aug18slogan.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMPANY</th>\n",
       "      <th>SLOGAN</th>\n",
       "      <th>SectorId</th>\n",
       "      <th>URL</th>\n",
       "      <th>Telecommunication Services</th>\n",
       "      <th>GICS_SubIndustry</th>\n",
       "      <th>WORDS</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3M Company</td>\n",
       "      <td>Science. Applied to life.</td>\n",
       "      <td>20</td>\n",
       "      <td>3M Company</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Conglomerates</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A&amp;P</td>\n",
       "      <td>At the A&amp;P, we watch our P's and Q's.</td>\n",
       "      <td>30</td>\n",
       "      <td>A&amp;P</td>\n",
       "      <td>Consumer Staples</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abbott Laboratories</td>\n",
       "      <td>Life. To the fullest.</td>\n",
       "      <td>35</td>\n",
       "      <td>Abbott Laboratories</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AbbVie</td>\n",
       "      <td>People. Passion. Possibilities.</td>\n",
       "      <td>35</td>\n",
       "      <td>AbbVie Inc.</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Pharmaceuticals</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABN AMRO Bank</td>\n",
       "      <td>Making More Possible</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Financials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               COMPANY                                 SLOGAN  SectorId  \\\n",
       "0           3M Company              Science. Applied to life.        20   \n",
       "1                  A&P  At the A&P, we watch our P's and Q's.        30   \n",
       "2  Abbott Laboratories                  Life. To the fullest.        35   \n",
       "3               AbbVie        People. Passion. Possibilities.        35   \n",
       "4       ABN AMRO Bank                    Making More Possible        40   \n",
       "\n",
       "                   URL Telecommunication Services          GICS_SubIndustry  \\\n",
       "0           3M Company                Industrials  Industrial Conglomerates   \n",
       "1                  A&P           Consumer Staples                       NaN   \n",
       "2  Abbott Laboratories                Health Care     Health Care Equipment   \n",
       "3          AbbVie Inc.                Health Care           Pharmaceuticals   \n",
       "4                  NaN                 Financials                       NaN   \n",
       "\n",
       "   WORDS Unnamed: 7  \n",
       "0    4.0        NaN  \n",
       "1    NaN             \n",
       "2    4.0        NaN  \n",
       "3    3.0        NaN  \n",
       "4    NaN        NaN  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['SLOGAN']\n",
    "y = df['SectorId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cleaning_text() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-ab54d66c14eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-12622e2ab615>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0muser\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mclean_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleaning_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cleaning_text() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "nlp.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CSLOGANS'] = clean_text(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('toto.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
