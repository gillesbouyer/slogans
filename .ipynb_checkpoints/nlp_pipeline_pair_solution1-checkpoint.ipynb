{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an NLP Pipeline Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the pair problem today, you're going to work on building a class that manages your NLP needs. The goal is to build something that takes in a bunch of classes, and can spit out the cleaned text as a vector. I'll get you started with a template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T13:13:54.921112Z",
     "start_time": "2018-08-13T13:13:54.918145Z"
    }
   },
   "outputs": [],
   "source": [
    "class nlp_pipe:\n",
    "    \n",
    "    def __init__(self, vectorizer, cleaning_function, tokenizer, stemmer):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.cleaning_function = cleaning_function\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "    \n",
    "    def fit(self, text_to_fit_on):\n",
    "        pass\n",
    "    \n",
    "    def transfrom(self, text_to_clean):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick note, if you want to pass a function into a class you can do so like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T13:15:31.601377Z",
     "start_time": "2018-08-13T13:15:31.597856Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_the_word_bob_three_times():\n",
    "    for i in range(3):\n",
    "        print('bob')\n",
    "        \n",
    "        \n",
    "class this_is_an_example:\n",
    "    \n",
    "    def __init__(self, function_input):\n",
    "        self.function_to_run = function_input\n",
    "        \n",
    "    def do_the_thing(self):\n",
    "        self.function_to_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T13:15:46.464728Z",
     "start_time": "2018-08-13T13:15:46.461852Z"
    }
   },
   "outputs": [],
   "source": [
    "example = this_is_an_example(print_the_word_bob_three_times)\n",
    "# Note that when I put the function in, I didn't invoke it with the parens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T13:15:51.402550Z",
     "start_time": "2018-08-13T13:15:51.399058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bob\n",
      "bob\n",
      "bob\n"
     ]
    }
   ],
   "source": [
    "example.do_the_thing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what I want is the ability to do something like:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nlp = nlp_pipe(CountVectorizer(), simple_cleaning_function_i_made, TreebankWordTokenizer(), PorterStemmer())\n",
    "nlp.fit(train_corpus)\n",
    "nlp.transform(test_corpus)\n",
    "```\n",
    "Which should return the test corpus in its vectorizer format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T13:24:30.903175Z",
     "start_time": "2018-08-13T13:24:30.893116Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "\n",
    "\n",
    "class nlp_preprocessor:\n",
    "   \n",
    "    def __init__(self, vectorizer=CountVectorizer(), tokenizer=None, \n",
    "                 cleaning_function=None, stemmer=None):\n",
    "        \"\"\"\n",
    "        A class for pipelining our data in NLP problems. The user provides a series of \n",
    "        tools, and this class manages all of the training, transforming, and modification\n",
    "        of the text data.\n",
    "        ---\n",
    "        Inputs:\n",
    "        vectorizer: the model to use for vectorization of text data\n",
    "        tokenizer: The tokenizer to use, if none defaults to split on spaces\n",
    "        cleaning_function: how to clean the data, if None, defaults to the in built class\n",
    "        \"\"\"\n",
    "        if not tokenizer:\n",
    "            tokenizer = self.splitter\n",
    "        if not cleaning_function:\n",
    "            cleaning_function = self.clean_text\n",
    "        self.stemmer = stemmer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.cleaning_function = cleaning_function\n",
    "        self.vectorizer = vectorizer\n",
    "        self._is_fit = False\n",
    "        \n",
    "    def splitter(self, text):\n",
    "        \"\"\"\n",
    "        Default tokenizer that splits on spaces naively\n",
    "        \"\"\"\n",
    "        return text.split(' ')\n",
    "        \n",
    "    def clean_text(self, text, tokenizer, stemmer):\n",
    "        \"\"\"\n",
    "        A naive function to lowercase all works can clean them quickly.\n",
    "        This is the default behavior if no other cleaning function is specified\n",
    "        \"\"\"\n",
    "        cleaned_text = []\n",
    "        for post in text:\n",
    "            cleaned_words = []\n",
    "            for word in tokenizer(post):\n",
    "                low_word = word.lower()\n",
    "                if stemmer:\n",
    "                    low_word = stemmer.stem(low_word)\n",
    "                cleaned_words.append(low_word)\n",
    "            cleaned_text.append(' '.join(cleaned_words))\n",
    "        return cleaned_text\n",
    "    \n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        Cleans the data and then fits the vectorizer with\n",
    "        the user provided text\n",
    "        \"\"\"\n",
    "        clean_text = self.cleaning_function(text, self.tokenizer, self.stemmer)\n",
    "        self.vectorizer.fit(clean_text)\n",
    "        self._is_fit = True\n",
    "        \n",
    "    def transform(self, text):\n",
    "        \"\"\"\n",
    "        Cleans any provided data and then transforms the data into\n",
    "        a vectorized format based on the fit function. Returns the\n",
    "        vectorized form of the data.\n",
    "        \"\"\"\n",
    "        if not self._is_fit:\n",
    "            raise ValueError(\"Must fit the models before transforming!\")\n",
    "        clean_text = self.cleaning_function(text, self.tokenizer, self.stemmer) # ????????? not defined here\n",
    "        return self.vectorizer.transform(clean_text)\n",
    "    \n",
    "    def save_pipe(self, filename):\n",
    "        \"\"\"\n",
    "        Writes the attributes of the pipeline to a file\n",
    "        allowing a pipeline to be loaded later with the\n",
    "        pre-trained pieces in place.\n",
    "        \"\"\"\n",
    "        if type(filename) != str:\n",
    "            raise TypeError(\"filename must be a string\")\n",
    "        pickle.dump(self.__dict__, open(filename+\".mdl\",'wb'))\n",
    "        \n",
    "    def load_pipe(self, filename):\n",
    "        \"\"\"\n",
    "        Writes the attributes of the pipeline to a file\n",
    "        allowing a pipeline to be loaded later with the\n",
    "        pre-trained pieces in place.\n",
    "        \"\"\"\n",
    "        if type(filename) != str:\n",
    "            raise TypeError(\"filename must be a string\")\n",
    "        if filename[-4:] != '.mdl':\n",
    "            filename += '.mdl'\n",
    "        self.__dict__ = pickle.load(open(filename,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T13:31:22.431182Z",
     "start_time": "2018-08-13T13:31:22.427759Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = ['BOB the builder', 'is a strange', 'caRtoon type thing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T13:33:06.286865Z",
     "start_time": "2018-08-13T13:33:06.282091Z"
    }
   },
   "outputs": [],
   "source": [
    "def simple_cleaning_function_i_made(text, tokenizer, stemmer):\n",
    "    cleaned_text = []\n",
    "    for post in text:\n",
    "        cleaned_words = []\n",
    "        for word in tokenizer(post):\n",
    "            low_word = word.lower()\n",
    "            if stemmer:\n",
    "                low_word = stemmer.stem(low_word)\n",
    "            cleaned_words.append(low_word)\n",
    "        cleaned_text.append(' '.join(cleaned_words))\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T13:33:11.089726Z",
     "start_time": "2018-08-13T13:33:11.079610Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a7699ae13d7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m nlp = nlp_preprocessor(vectorizer=TfidfVectorizer(min_df=0.3, max_df=0.8), \n\u001b[1;32m      6\u001b[0m                        \u001b[0mcleaning_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msimple_cleaning_function_i_made\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                        tokenizer=TreebankWordTokenizer().tokenize, stemmer=PorterStemmer())\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mvectorized_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e3f4074f10af>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vectorizer, tokenizer, cleaning_function, stemmer)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleaning_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaning_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nlp = nlp_preprocessor(vectorizer=TfidfVectorizer(min_df=0.3, max_df=0.8), \n",
    "                       cleaning_function=simple_cleaning_function_i_made, \n",
    "                       tokenizer=TreebankWordTokenizer().tokenize, stemmer=PorterStemmer())\n",
    "nlp.fit(corpus)\n",
    "vectorized_docs = nlp.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers)\n",
    "from the store. Should I pick up some black-eyed peas as well?\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "class nlp_preprocesseur:\n",
    "\n",
    "    def __init__(self, tokenizer=None):\n",
    "        \"\"\"\n",
    "        A class to prep data. \n",
    "        Tokenize\n",
    "        Clean\n",
    "            #    mispelling, remove numbers, remove special characters, go lowercase, remove stop words, chunking\n",
    "            #        name entity recognition,coumpound term extraction\n",
    "            #    stemming\n",
    "            #    lemmatization\n",
    "            #    POS Tagging\n",
    "        Vectorize\n",
    "        \"\"\"\n",
    "        if not tokenizer:\n",
    "            tokenizer = self.tokenizeSentence\n",
    "        elif tokenizer == \"word\":\n",
    "            tokenizer = self.tokenizeWord\n",
    "        elif tokenizer == \"space\":\n",
    "            tokenizer = self.tokenizeSpace\n",
    "        self.tokenizer = tokenizer    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def tokenizeSentence(self,my_text):\n",
    "        return sent_tokenize(my_text)\n",
    "\n",
    "    def tokenizeWord(self,my_text):        # each ) . ? is a word\n",
    "        return word_tokenize(my_text)\n",
    "\n",
    "    def tokenizeSpace(self,my_text):\n",
    "        return my_text.split(' ')  # keeps the blanks at the end of sentence\n",
    "            # regex, N-gram\n",
    "\n",
    "    def tokenizeBigrams(self,my_text):\n",
    "        my_words = self.tokenizeWord(my_text) \n",
    "        return list(ngrams(my_words,2)) \n",
    "\n",
    "        \n",
    "    def tokenizeChar(self,my_text):  # stops at the last non blank character\n",
    "        caracteres = \"\\s+\"\n",
    "        return RegexpTokenizer(caracteres, gaps=True).tokenize(my_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = nlp_preprocesseur()\n",
    "nlp3 = nlp_preprocesseur(tokenizer = \"word\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'tokenizeBigrams',\n",
       " 'tokenizeChar',\n",
       " 'tokenizeSentence',\n",
       " 'tokenizeSpace',\n",
       " 'tokenizeWord',\n",
       " 'tokenizer']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nlp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi Mr. Smith!',\n",
       " 'I’m going to buy some vegetables (tomatoes and cucumbers)\\nfrom the store.',\n",
       " 'Should I pick up some black-eyed peas as well?']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2.tokenizer(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'Mr.',\n",
       " 'Smith',\n",
       " '!',\n",
       " 'I',\n",
       " '’',\n",
       " 'm',\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'some',\n",
       " 'vegetables',\n",
       " '(',\n",
       " 'tomatoes',\n",
       " 'and',\n",
       " 'cucumbers',\n",
       " ')',\n",
       " 'from',\n",
       " 'the',\n",
       " 'store',\n",
       " '.',\n",
       " 'Should',\n",
       " 'I',\n",
       " 'pick',\n",
       " 'up',\n",
       " 'some',\n",
       " 'black-eyed',\n",
       " 'peas',\n",
       " 'as',\n",
       " 'well',\n",
       " '?']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp3.tokenizer(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'Mr.',\n",
       " 'Smith!',\n",
       " 'I’m',\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'some',\n",
       " 'vegetables',\n",
       " '(tomatoes',\n",
       " 'and',\n",
       " 'cucumbers)\\nfrom',\n",
       " 'the',\n",
       " 'store.',\n",
       " 'Should',\n",
       " 'I',\n",
       " 'pick',\n",
       " 'up',\n",
       " 'some',\n",
       " 'black-eyed',\n",
       " 'peas',\n",
       " 'as',\n",
       " 'well?\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2.tokenizeSpace(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'Mr.',\n",
       " 'Smith!',\n",
       " 'I’m',\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'some',\n",
       " 'vegetables',\n",
       " '(tomatoes',\n",
       " 'and',\n",
       " 'cucumbers)',\n",
       " 'from',\n",
       " 'the',\n",
       " 'store.',\n",
       " 'Should',\n",
       " 'I',\n",
       " 'pick',\n",
       " 'up',\n",
       " 'some',\n",
       " 'black-eyed',\n",
       " 'peas',\n",
       " 'as',\n",
       " 'well?']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2.tokenizeChar(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'Mr.',\n",
       " 'Smith',\n",
       " '!',\n",
       " 'I',\n",
       " '’',\n",
       " 'm',\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'some',\n",
       " 'vegetables',\n",
       " '(',\n",
       " 'tomatoes',\n",
       " 'and',\n",
       " 'cucumbers',\n",
       " ')',\n",
       " 'from',\n",
       " 'the',\n",
       " 'store',\n",
       " '.',\n",
       " 'Should',\n",
       " 'I',\n",
       " 'pick',\n",
       " 'up',\n",
       " 'some',\n",
       " 'black-eyed',\n",
       " 'peas',\n",
       " 'as',\n",
       " 'well',\n",
       " '?']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2.tokenizeWord(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hi', 'Mr.'),\n",
       " ('Mr.', 'Smith'),\n",
       " ('Smith', '!'),\n",
       " ('!', 'I'),\n",
       " ('I', '’'),\n",
       " ('’', 'm'),\n",
       " ('m', 'going'),\n",
       " ('going', 'to'),\n",
       " ('to', 'buy'),\n",
       " ('buy', 'some'),\n",
       " ('some', 'vegetables'),\n",
       " ('vegetables', '('),\n",
       " ('(', 'tomatoes'),\n",
       " ('tomatoes', 'and'),\n",
       " ('and', 'cucumbers'),\n",
       " ('cucumbers', ')'),\n",
       " (')', 'from'),\n",
       " ('from', 'the'),\n",
       " ('the', 'store'),\n",
       " ('store', '.'),\n",
       " ('.', 'Should'),\n",
       " ('Should', 'I'),\n",
       " ('I', 'pick'),\n",
       " ('pick', 'up'),\n",
       " ('up', 'some'),\n",
       " ('some', 'black-eyed'),\n",
       " ('black-eyed', 'peas'),\n",
       " ('peas', 'as'),\n",
       " ('as', 'well'),\n",
       " ('well', '?')]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2.tokenizeBigrams(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
