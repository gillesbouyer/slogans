{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook creates the base code for an app to pick a bukowski poem verse to associate with a slogan \n",
    "## -1- creating a class for a pipeline\n",
    "## -2- creating a function to call the class\n",
    "## -3- enter the slogan --> return the associated verse \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start -1- creating a class for a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries needed in the notebook\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries needed in the class\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class text_processor:\n",
    "\n",
    "    def __init__(self, remover_function=None, tokenizer_function=None, \n",
    "                 cleaning_function=None, stemmer_function=None,\n",
    "                     vectorizer_function = CountVectorizer()):\n",
    "        self.remover = remover_function\n",
    "        self.tokenizer = tokenizer_function\n",
    "        self.cleaner = cleaning_function\n",
    "        self.stemmer = stemmer_function\n",
    "        self.vectorizer = vectorizer_function\n",
    "# The variables in the init are to:\n",
    "#        remove characters from the text, \n",
    "#        tokenize the text, \n",
    "#        clean the text(typically lowercase but could be other actions like pos)\n",
    "#        stem the words\n",
    "#        vectorize\n",
    "#  the values go into variables to be used later in the class\n",
    "#  below are some test to decided which function to use in the class\n",
    "        if remover_function == 'no_punctuation':\n",
    "            self.remover = self.no_punctuation\n",
    "        if tokenizer_function == 'tk_word':\n",
    "            self.tokenizer = self.tk_word\n",
    "        if not tokenizer_function:\n",
    "            self.tokenizer = self.splitter\n",
    "        if cleaning_function == 'lowstem':\n",
    "            self.cleaner = self.lowstem\n",
    "                \n",
    "   # cleaning functions\n",
    "\n",
    "    def lower(self,X):\n",
    "        sentences = []\n",
    "        for sentence in X:\n",
    "            sentences.append(sentence.lower()) \n",
    "        return sentences\n",
    "\n",
    "\n",
    "    def no_punctuation(self,X):\n",
    "    # remove the punctuation\n",
    "        pos = []\n",
    "        for sentence in X:\n",
    "            for punc in string.punctuation:\n",
    "                sentence = sentence.replace(punc,'')\n",
    "            pos.append(sentence)\n",
    "        return pos\n",
    "    \n",
    " # tokenizer functions   \n",
    "    \n",
    "    def tk_word(self,X):\n",
    "        vocabulary = []\n",
    "        for x in X:\n",
    "            vocabulary.append(word_tokenize(x)) \n",
    "        return vocabulary        \n",
    "    \n",
    "    \n",
    "    def splitter(self, text):\n",
    "        \"\"\"\n",
    "        Default tokenizer that splits on spaces naively\n",
    "        \"\"\"\n",
    "        return text.split(' ')\n",
    "\n",
    "   # stemmer function\n",
    "    \n",
    "\n",
    "    def stem(self,X):\n",
    "        stemmed = []\n",
    "        for word in (X):\n",
    "            stem_word = stemmer.stem(word)\n",
    "            stemmed.append(stem_word)\n",
    "        return stemmed\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # vectorizing function. Decide to do the fit and the run of it\n",
    "    def vectorize(self, X):\n",
    "        self.vectorizer.fit(X)\n",
    "        self.columns=self.vectorizer.get_feature_names()  # the names of all the words used in vectorization\n",
    "        return self.vectorizer.transform(X).toarray()\n",
    "        \n",
    "     # the fit covers text changes to vectorization of it.   \n",
    "    def fit(self,X):\n",
    "        clear_text = self.remover(X)\n",
    "        clear_text = self.lower(clear_text)\n",
    "#        clear_text = self.stem(clear_text)\n",
    "        self.matrix = self.vectorize(clear_text)\n",
    " # i create my matrix of 0 and 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end -1- creating a class for a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start -2- creating a function to use the class\n",
    "# and to find the bukowski verse the closest to the slogan on principal component one of the LSA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(text):\n",
    "    \n",
    "    nlp = text_processor(remover_function='no_punctuation',tokenizer_function = 'tk_word'\n",
    "                    ,stemmer_function = PorterStemmer,\n",
    "# this removes most records !                    vectorizer_function=TfidfVectorizer(min_df=0.3, max_df=0.8))\n",
    "                    vectorizer_function=TfidfVectorizer(min_df=0, max_df=1))\n",
    "    \n",
    "    df = pd.read_excel('./data/bukowski1.xlsx')\n",
    "    X = df['verses']\n",
    "# got data from around 30 Bukowski poems on line and paste them in the excel\n",
    " # got them in a dataframe    \n",
    "    pos = [text]\n",
    "    for x in X:\n",
    "        for punc in string.punctuation:\n",
    "            x = x.replace(punc,'')\n",
    "        pos.append(x)\n",
    "# put them all in one list\n",
    "# create the class that vectorize everything and gives me the columns and the matrix of 0/1\n",
    "    nlp.fit(pos)\n",
    "    pos_matrix = nlp.matrix\n",
    "    pos_columns = nlp.columns\n",
    "# call LSA   \n",
    "    lsa = TruncatedSVD(2, algorithm = 'arpack')\n",
    "    dtm_lsa = lsa.fit_transform(pos_matrix)\n",
    "    dtm_lsa = Normalizer(copy=False).fit_transform(dtm_lsa)\n",
    "# Load results in dataframe\n",
    "    pd.DataFrame(lsa.components_.round(5),\n",
    "                 index = [\"component_1\",\"component_2\"],columns = pos_columns)\n",
    "    df3 = pd.DataFrame(dtm_lsa.round(5), index = pos, columns = [\"component_1\",\"component_2\" ])\n",
    "# create another dataframe ordered by component 1 len(df3) is keep all \n",
    "    df4 = df3.nlargest(len(df3), 'component_1')\n",
    "#   df index is the verses and one slogan. we reindex to get all the line numbered be\n",
    "#   cause we want to take the verse following the slogan in the list\n",
    "    df4 = df4.reset_index()\n",
    "    mask1 = (df4['index'] == text)\n",
    "    n = df4[mask1].index[0]    # this is the position of the slogan ordered in component 1 list\n",
    "    df5 = df3.nlargest(len(df3), 'component_2')\n",
    "    df5 = df5.reset_index()\n",
    "    mask5 = (df5['index'] == text)\n",
    "    m = df5[mask5].index[0]   #  this is the position of the slogan ordered in component 2 list\n",
    "# I chose only one. somehow component 1 looked the best\n",
    "    return (df4.iloc[n+1]['index']) \n",
    "# this return the verse (column name is index at position)\n",
    "#(n+1 is the position of the slogan+1 for the 1st item after the slogan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verse():\n",
    "    x = input()\n",
    "    return generator(x)\n",
    "# basic function to enter a slogan ... or whatever ... and return a verse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first in food\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' I decided never to become an American'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verse()  # calling the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
